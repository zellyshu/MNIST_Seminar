{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__future__ 모듈 \n",
    "파이썬 2.x버전이 print가 키워드라 함수처럼 처리가 안되어 함수로 전환해서 사용할 수 있게 만들어줌. 따라서 파이썬 3버전의 print 함수를 사용할 수 있게됨.\n",
    "\n",
    "argparse\n",
    ": 쉽게 인자값을 처리할 수 있도록 도와주는 라이브러리\n",
    "\n",
    "sys\n",
    ": 파이썬 인터프리터와 관련된 정보와 기능을 제공하는 모듈. 실행 시 인자 값을 받을 수 있음.\n",
    "\n",
    "input_data로 mnist 데이터를 받아온다.\n",
    "tensorflow를 임포트 한다.\n",
    "\n",
    "FLAGS\n",
    ": 프로젝트 전역으로 변수 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    # Import data\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "    \n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.matmul(x, W) + b\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # Train\n",
    "    for _ in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "    # Test trained model\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main 메소드를 불러온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수들을 설정한다.\n",
    "\n",
    "placeholder \n",
    "\n",
    "--> 우리가 텐서플로우에서 연산을 실행할 때 값을 입력할 자리.\n",
    "\n",
    "--> 여기서는 '784'차원의 벡터로 변형된 MNIST이미지의 데이터를 넣으려고 함!\n",
    "\n",
    "--> [None, 784]의 형태를 갖고 부동소수점으로 이루어진 2차원 텐서로 표현. (None은 해당 차원의 길이가 어떤 길이든지 될 수 있음을 의미!)\n",
    "\n",
    "모델에는 가중치와 바이어스 역시 필요!\n",
    "\n",
    "W / b\n",
    "\n",
    "--> 둘 다 0으로 이루어진 텐서로 초기화. 이제부터 W 와 b 를 학습해 나갈 것이므로, 각각의 초기값은 크게 중요X\n",
    "\n",
    "Q. W가 [784, 10]의 형태를 왜 가지는 걸까?\n",
    "\n",
    "A. W 에 784 차원의 이미지 벡터를 곱해서 각 클래스에 대한 증거값을 나타내는 10차원 벡터를 얻고자 하기 때문!\n",
    "\n",
    "y\n",
    "\n",
    "--> matmul로 x와 W를 곱해준다. (matmul은 행렬을 곱해줌) \n",
    "--> 거기에 b를 더한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_\n",
    "\n",
    "cross entropy를 구현하기 위해 올바른 답을 넣기 위한 새로운 placeholder를 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제에서는 cross_entropy를 조금 다르게 구현하고 있지만, 예제에서의 방법은 수치적으로 불안정 할 수가 있기에 여기서는 위와 같이 구현.\n",
    "\n",
    "cross_entropy\n",
    "\n",
    "--> 즉, tf.matmul(x, W)+b에 tf.nn.softmax_cross_entropy_with_logits을 사용하고, batch 전반에 걸쳐 평균을 구하게 된다는 이야기. \n",
    "\n",
    "--> Why? 이 방법이 더욱 수치적으로 안정된 함수로서 softmax activation을 내부적으로 계산해주기 때문.\n",
    "\n",
    "train_step\n",
    "\n",
    "--> 텐서플로우에게 학습 비율 0.5로 경사 하강법을 적용하여 cross entropy를 최소화하도록 지시. \n",
    "\n",
    "* 경사하강법 : 텐서플로우가 각각의 변수의 비용을 줄이는 방향으로 조금씩 이동시키는 매우 단순한 방법\n",
    "\n",
    "sess를 초기화하고, 작성한 변수들을 초기화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 시켜보자. 여기서는 학습을 1000회 실시하게 된다.\n",
    "\n",
    "반복되는 루프의 각 단계마다, train(학습) 데이터셋에서 무작위로 선택된 100개의 데이터로 구성된 batch를 가져오게 되고, 그 다음엔 placeholder의 자리에 데이터를 넣을 수 있도록 train_step을 실행하여 batch 데이터를 넘긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델이 얼마나 정확한지를 출력. (모델 평가)\n",
    "\n",
    "* correct_prediction\n",
    "\n",
    "tf.argmax\n",
    "\n",
    "--> 텐서 안에서 특정 축을 따라 가장 큰 값의 인덱스를 찾기에 매우 유용한 함수\n",
    "예를 들어, tf.argmax(y,1)은 우리의 모델이 생각하기에 각 데이터에 가장 적합하다고 판단한 가장 증거값이 큰 레이블이며, tf.argmax(y_,1) 은 실제 레이블이다. \n",
    "\n",
    "tf.equal\n",
    "\n",
    "--> 을 사용하여 학습된 모델이 예측한 값이 실제 값과 맞았는지 확인할 수 있다.\n",
    "\n",
    "이렇게 하면, boolean 값으로 이루어진 리스트를 얻게 된다.\n",
    "\n",
    "* accuracy\n",
    "\n",
    "얼마나 많이 맞았는지 판단하려면, 이 값을 부동 소수점 값으로 변환한 후 평균을 계산하면 됨!\n",
    "ex. [True, False, True, True] 를 [1, 0, 1, 1]로 변환할 수 있고, 이 값의 평균을 계산하면 \"0.75\" 가 된다.\n",
    "\n",
    "* print\n",
    "\n",
    "테스트 데이터를 대상으로 정확도를 계산 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메소드를 불러와서 실제로 실행 ! \n",
    "짜잔~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "0.9191\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ko/anaconda3/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parser = argparse.ArgumentParser()\n",
    "\n",
    "--> parser 객체를 생성하는 코드. 이 코드를 실행하고 parser 뒤에 .을 붙여 함수들을 사용한다.\n",
    "\n",
    "* parser.add_argument()\n",
    "\n",
    "--> $python a.py처럼 실행할 때, 위 옵션을 사용하려면 $python a.py --data_dir /tmp/tensorflow/mnist/input_data 처럼 옵션 설정하여 사용 가능.\n",
    "--> 여기서는 default 옵션을 사용해서 default 디렉토리와 다른 디렉토리에 데이터가 있지 않은 이상 굳이 명시하지 않아주어도 무방\n",
    "--> help 옵션은 터미널에서 $python a.pu -h 를 실행했을 때, help 문구가 출력되면서 옵션에 대한 설명을 나타내게 된다. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
